---
title: "FLASH ATTENTION T5 (FAT5)"
tags:
  - NLP
  - FAT5
  - Flash Attention
  - T5
  - "2024"
excerpt : "NLP - Flash Attention extended to T5 via CUDA and Triton kernels<br>- Difficulty: advanced"
header:
   overlay_color: "#1C2A4D"
author_profile: false
translation: "FAT5/"
sidebar:
    nav: sidebar-nlp-en
classes: wide
---

<p style="text-align:justify;">
While much effort has been devoted to optimising decoder transformers, thus abandoning the encoder, we believe it is essential to maintain an encoder-decoder architecture.<br>
Indeed, this architecture, which offers interesting performance for <a href="http://arxiv.org/abs/2306.04757">instruction tuning</a>, is suitable for <a href="https://arxiv.org/abs/2305.02301">distillation</a> and seems superior to decoder models when <a href="https://arxiv.org/abs/2402.00841">finetuned</a>. It has also been <a href="https://arxiv.org/abs/2204.05832">shown</a> that encoder-decoder models trained with masked language modelling achieve better zero-shot performance after multitasking finetuning compared with a decoder model.<br>
Beyond NLP, which is the focus of this blog post, encoder-decoder architecture is widely used in other fields such as audio or time series, for example. The encoder of such architecture is also used in some diffusion models.<br>
That's why we've decided to focus on the <a href="https://jmlr.org/papers/v21/20-074.html">T5</a>.<br><br>

This article presents the optimisations we have implemented to efficiently pre-train a T5 in French with 147M parameters in a reasonable time (1,461 H for 419B tokens) and with limited resources (1 single A100; i.e. a computing budget of around 2,200 euros). To achieve this, we designed CUDA/Triton kernels to make Flash Attention compatible with T5 and provide linear inference, thus extending the context size that can be taken into account by the model.
All the optimizations applied are detailed on <a href="https://hf.co/spaces/CATIE-AQ/FAT5-rapport">Hugging Face</a>.
</p>
<br>

<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/catie-aq/flashT5/main/assets/FAT5_dark.gif">
</figure>
</center>

<br>

<b>The pre-training code is available in our  <a href="https://github.com/catie-aq/flashT5">GitHub repository</a> under Apache-2.0 license and the weights of the model trained on our <a href="https://huggingface.co/CATIE-AQ">Hugging Face account</a>.</b>

<br>

<center>
    Read the full article on <a href="https://hf.co/spaces/CATIE-AQ/FAT5-report">Hugging Face</a>.
</center>

<br><br>
