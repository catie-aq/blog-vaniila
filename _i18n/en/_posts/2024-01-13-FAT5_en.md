---
title: "FLASH ATTENTION T5 (FAT5)"
tags:
  - NLP
  - FAT5
  - Flash Attention
  - T5
  - "2024"
excerpt : "NLP - Flash Attention extended to T5 via CUDA and Triton kernels<br>- Difficulty: advanced"
header:
   overlay_color: "#1C2A4D"
author_profile: false
translation: "FAT5/"
sidebar:
    nav: sidebar-nlp-en
classes: wide
---

While much effort has been devoted to optimising decoder transformers, thus abandoning the encoder, we believe it is essential to maintain an encoder-decoder architecture.<br>
Indeed, this architecture, which offers interesting performance for instruction tuning, is suitable for distillation and seems superior to decoder models when finetuned. 
It has also been shown that encoder-decoder models trained with masked language modelling achieve better zero-shot performance after multitasking finetuning compared with a decoder model. <br>
Beyond NLP, on which we have focused, encoder-decoder architecture is widely used in other fields such as audio or time series, for example.<br>
That's why we've decided to focus on the T5.


The optimizations implemented to efficiently pre-train a 147M parameter T5 in French in a reasonable time  (xx to xx days depending on the number of tokens shown to the model), and more generally the methodology applied,  are detailed in a blog post available on <a href="https://hf.co/spaces/CATIE-AQ/FAT5-report">Hugging Face</a>.<br>
In particular, we detail the design of CUDA/Triton kernels  to enable Flash Attention to be compatible with T5 and to provide linear inference, thus extending the context size that can be handled by the model.

<br>

<center>
<figure class="image">
  <img src="https://github.com/catie-aq/flashT5/blob/main/assets/FAT5_dark.gif">
</figure>
</center>

<br>

<b>The pre-training code is available in our  <a href="https://github.com/catie-aq/flashT5">GitHub repository</a> under Apache-2.0 license and the weights of the model trained on our <a href="https://huggingface.co/CATIE-AQ">Hugging Face account</a>.</b>

<br>

<center>
    Read the full article on <a href="https://hf.co/spaces/CATIE-AQ/FAT5-report">Hugging Face</a>.
</center>
