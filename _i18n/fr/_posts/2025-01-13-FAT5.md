---
title: "LE FLASH ATTENTION T5 (FAT5)"
tags:
  - NLP
  - FAT5
  - Flash Attention
  - T5
  - "2024"
excerpt : "NLP - Extension de la Flash Attention au T5 via des noyaux CUDA et Triton<br>- Difficulté : confirmé"
header:
   overlay_color: "#1C2A4D"
author_profile: false
translation: "en/FAT5_en/"
sidebar:
    nav: sidebar-nlp
classes: wide
---

<p style="text-align:justify;">
Alors que beaucoup d’efforts ont été consacrés à l’optimisation de <i>transformer</i> de type décodeur, abandonnant ainsi l’encodeur, nous pensons qu’il est essentiel de maintenir une architecture encodeur-décodeur.
En effet, cette architecture qui présente des performances intéressantes pour l’<a href="http://arxiv.org/abs/2306.04757">instruction tuning</a>, est propice à la <a href="https://arxiv.org/abs/2305.02301">distillation</a> et semble supérieure aux modèles décodeur lorsqu’elle est <a href="https://arxiv.org/abs/2402.00841">finetunée</a>.
Il a aussi été <a href="https://arxiv.org/abs/2204.05832">montré</a> que les modèles encodeur-décodeur entraînés avec une modélisation du langage masqué obtiennent une meilleure performance zéro-<i>shot</i> après un <i>finetuning</i> multitâche par rapport à un modèle décodeur. L'encodeur d'une telle architecture est également utilisé dans des modèles de diffusion.<br>
Au-delà du NLP sur lequel nous nous sommes concentrés, l’architecture encodeur-décodeur est très utilisée dans d’autres domaines comme l’audio ou les séries temporelles par exemple.<br>
Dans cette logique, nous avons décidé de nous concentrer sur le <a href="https://jmlr.org/papers/v21/20-074.html">T5</a>.<br><br>

Dans cet article, sont détaillons les optimisations que nous avons mises en place afin de pré-entraîner de manière efficiente un T5 de 147M de paramètres en français en un temps raisonnable (1 461 H pour 419Mds de <i>tokens</i>) et avec des moyens limités (1 A100 ; soit un budget de calcul d'environ 2 200 euros). Pour ce faire, nous avons conçu des noyaux CUDA/Triton afin de rendre la Flash Attention compatible avec T5 et de fournir une inférence linéaire, étendant ainsi la taille du contexte qui peut être prise en compte par le modèle. L'ensemble des optimisations appliquées sont détaillées sur <a href="https://hf.co/spaces/CATIE-AQ/FAT5-rapport">Hugging Face</a>.
</p>
<br>

<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/catie-aq/flashT5/main/assets/FAT5_dark.gif">
</figure>
</center>

<br>

<b>Le code de pré-entrainement est disponible sur notre <a href="https://github.com/catie-aq/flashT5">répertoire GitHub</a> sous licence Apache-2.0 
et les poids du modèle entraîné sur notre compte <a href="https://huggingface.co/CATIE-AQ">Hugging Face</a>.</b>

<br>

<center>
    Lire l'article complet sur <a href="https://hf.co/spaces/CATIE-AQ/FAT5-rapport">Hugging Face</a>.
</center>

<br><br>
